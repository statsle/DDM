[32m[05/17 06:44:07 graph]: [0mRank of current process: 0. World size: 1
[32m[05/17 06:44:12 graph]: [0mEnvironment info:
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-90-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 495.44
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==1.10.0+cu113
[pip3] torchvision==0.11.1+cu113
[conda] blas                      1.0                         mkl    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl                       2021.4.0           h06a4308_640    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0            py38h7f8727e_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.1            py38hd3c417c_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.2            py38h51133e4_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.21.4                   pypi_0    pypi
[conda] numpy-base                1.23.5           py38h31eccc5_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] torch                     1.10.0+cu113             pypi_0    pypi
[conda] torchvision               0.11.1+cu113             pypi_0    pypi
        Pillow (8.4.0)
[32m[05/17 06:44:12 graph]: [0mCommand line arguments: Namespace(local_rank=0, resume=False, seed=1234, start_epoch=0)
[32m[05/17 06:44:12 graph]: [0mRun 0th for seed 0
[32m[05/17 06:44:12 graph]: [0mTotal trainable params num : 7813551
[32m[05/17 06:44:19 graph]: [0m----------Start Training----------
[32m[05/17 06:44:20 graph]: [0mstart epoch 0.
[32m[05/17 06:44:20 graph]: [0m# Epoch 0: train_loss: 1.0220 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 1.
[32m[05/17 06:44:20 graph]: [0m# Epoch 1: train_loss: 0.8043 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 2.
[32m[05/17 06:44:20 graph]: [0m# Epoch 2: train_loss: 0.6895 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 3.
[32m[05/17 06:44:20 graph]: [0m# Epoch 3: train_loss: 0.6362 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 4.
[32m[05/17 06:44:20 graph]: [0m# Epoch 4: train_loss: 0.6087 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 5.
[32m[05/17 06:44:20 graph]: [0m# Epoch 5: train_loss: 0.5906 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 6.
[32m[05/17 06:44:20 graph]: [0m# Epoch 6: train_loss: 0.5791 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 7.
[32m[05/17 06:44:20 graph]: [0m# Epoch 7: train_loss: 0.5704 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 8.
[32m[05/17 06:44:20 graph]: [0m# Epoch 8: train_loss: 0.5629 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 9.
[32m[05/17 06:44:20 graph]: [0m# Epoch 9: train_loss: 0.5555 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 10.
[32m[05/17 06:44:20 graph]: [0m# Epoch 10: train_loss: 0.5471 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 11.
[32m[05/17 06:44:20 graph]: [0m# Epoch 11: train_loss: 0.5397 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 12.
[32m[05/17 06:44:20 graph]: [0m# Epoch 12: train_loss: 0.5321 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 13.
[32m[05/17 06:44:20 graph]: [0m# Epoch 13: train_loss: 0.5265 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 14.
[32m[05/17 06:44:20 graph]: [0m# Epoch 14: train_loss: 0.5206 | lr: 0.000411
[32m[05/17 06:44:20 graph]: [0mstart epoch 15.
[32m[05/17 06:44:21 graph]: [0m# Epoch 15: train_loss: 0.5168 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 16.
[32m[05/17 06:44:21 graph]: [0m# Epoch 16: train_loss: 0.5125 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 17.
[32m[05/17 06:44:21 graph]: [0m# Epoch 17: train_loss: 0.5083 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 18.
[32m[05/17 06:44:21 graph]: [0m# Epoch 18: train_loss: 0.5040 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 19.
[32m[05/17 06:44:21 graph]: [0m# Epoch 19: train_loss: 0.5003 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 20.
[32m[05/17 06:44:21 graph]: [0m# Epoch 20: train_loss: 0.4973 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 21.
[32m[05/17 06:44:21 graph]: [0m# Epoch 21: train_loss: 0.4947 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 22.
[32m[05/17 06:44:21 graph]: [0m# Epoch 22: train_loss: 0.4916 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 23.
[32m[05/17 06:44:21 graph]: [0m# Epoch 23: train_loss: 0.4892 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 24.
[32m[05/17 06:44:21 graph]: [0m# Epoch 24: train_loss: 0.4858 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 25.
[32m[05/17 06:44:21 graph]: [0m# Epoch 25: train_loss: 0.4834 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 26.
[32m[05/17 06:44:21 graph]: [0m# Epoch 26: train_loss: 0.4807 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 27.
[32m[05/17 06:44:21 graph]: [0m# Epoch 27: train_loss: 0.4782 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 28.
[32m[05/17 06:44:21 graph]: [0m# Epoch 28: train_loss: 0.4777 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 29.
[32m[05/17 06:44:21 graph]: [0m# Epoch 29: train_loss: 0.4742 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 30.
[32m[05/17 06:44:21 graph]: [0m# Epoch 30: train_loss: 0.4716 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 31.
[32m[05/17 06:44:21 graph]: [0m# Epoch 31: train_loss: 0.4697 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 32.
[32m[05/17 06:44:21 graph]: [0m# Epoch 32: train_loss: 0.4677 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 33.
[32m[05/17 06:44:21 graph]: [0m# Epoch 33: train_loss: 0.4661 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 34.
[32m[05/17 06:44:21 graph]: [0m# Epoch 34: train_loss: 0.4645 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 35.
[32m[05/17 06:44:21 graph]: [0m# Epoch 35: train_loss: 0.4628 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 36.
[32m[05/17 06:44:21 graph]: [0m# Epoch 36: train_loss: 0.4618 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 37.
[32m[05/17 06:44:21 graph]: [0m# Epoch 37: train_loss: 0.4612 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 38.
[32m[05/17 06:44:21 graph]: [0m# Epoch 38: train_loss: 0.4613 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 39.
[32m[05/17 06:44:21 graph]: [0m# Epoch 39: train_loss: 0.4585 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 40.
[32m[05/17 06:44:21 graph]: [0m# Epoch 40: train_loss: 0.4568 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 41.
[32m[05/17 06:44:21 graph]: [0m# Epoch 41: train_loss: 0.4556 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 42.
[32m[05/17 06:44:21 graph]: [0m# Epoch 42: train_loss: 0.4557 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 43.
[32m[05/17 06:44:21 graph]: [0m# Epoch 43: train_loss: 0.4551 | lr: 0.000411
[32m[05/17 06:44:21 graph]: [0mstart epoch 44.
[32m[05/17 06:44:22 graph]: [0m# Epoch 44: train_loss: 0.4531 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 45.
[32m[05/17 06:44:22 graph]: [0m# Epoch 45: train_loss: 0.4528 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 46.
[32m[05/17 06:44:22 graph]: [0m# Epoch 46: train_loss: 0.4540 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 47.
[32m[05/17 06:44:22 graph]: [0m# Epoch 47: train_loss: 0.4506 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 48.
[32m[05/17 06:44:22 graph]: [0m# Epoch 48: train_loss: 0.4522 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 49.
[32m[05/17 06:44:22 graph]: [0m# Epoch 49: train_loss: 0.4502 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 50.
[32m[05/17 06:44:22 graph]: [0m# Epoch 50: train_loss: 0.4503 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 51.
[32m[05/17 06:44:22 graph]: [0m# Epoch 51: train_loss: 0.4490 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 52.
[32m[05/17 06:44:22 graph]: [0m# Epoch 52: train_loss: 0.4485 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 53.
[32m[05/17 06:44:22 graph]: [0m# Epoch 53: train_loss: 0.4477 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 54.
[32m[05/17 06:44:22 graph]: [0m# Epoch 54: train_loss: 0.4467 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 55.
[32m[05/17 06:44:22 graph]: [0m# Epoch 55: train_loss: 0.4466 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 56.
[32m[05/17 06:44:22 graph]: [0m# Epoch 56: train_loss: 0.4459 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 57.
[32m[05/17 06:44:22 graph]: [0m# Epoch 57: train_loss: 0.4452 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 58.
[32m[05/17 06:44:22 graph]: [0m# Epoch 58: train_loss: 0.4445 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 59.
[32m[05/17 06:44:22 graph]: [0m# Epoch 59: train_loss: 0.4438 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 60.
[32m[05/17 06:44:22 graph]: [0m# Epoch 60: train_loss: 0.4433 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 61.
[32m[05/17 06:44:22 graph]: [0m# Epoch 61: train_loss: 0.4419 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 62.
[32m[05/17 06:44:22 graph]: [0m# Epoch 62: train_loss: 0.4415 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 63.
[32m[05/17 06:44:22 graph]: [0m# Epoch 63: train_loss: 0.4404 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 64.
[32m[05/17 06:44:22 graph]: [0m# Epoch 64: train_loss: 0.4402 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 65.
[32m[05/17 06:44:22 graph]: [0m# Epoch 65: train_loss: 0.4387 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 66.
[32m[05/17 06:44:22 graph]: [0m# Epoch 66: train_loss: 0.4387 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 67.
[32m[05/17 06:44:22 graph]: [0m# Epoch 67: train_loss: 0.4373 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 68.
[32m[05/17 06:44:22 graph]: [0m# Epoch 68: train_loss: 0.4376 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 69.
[32m[05/17 06:44:22 graph]: [0m# Epoch 69: train_loss: 0.4376 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 70.
[32m[05/17 06:44:22 graph]: [0m# Epoch 70: train_loss: 0.4411 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 71.
[32m[05/17 06:44:22 graph]: [0m# Epoch 71: train_loss: 0.4439 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 72.
[32m[05/17 06:44:22 graph]: [0m# Epoch 72: train_loss: 0.4383 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 73.
[32m[05/17 06:44:22 graph]: [0m# Epoch 73: train_loss: 0.4375 | lr: 0.000411
[32m[05/17 06:44:22 graph]: [0mstart epoch 74.
[32m[05/17 06:44:23 graph]: [0m# Epoch 74: train_loss: 0.4375 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 75.
[32m[05/17 06:44:23 graph]: [0m# Epoch 75: train_loss: 0.4343 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 76.
[32m[05/17 06:44:23 graph]: [0m# Epoch 76: train_loss: 0.4355 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 77.
[32m[05/17 06:44:23 graph]: [0m# Epoch 77: train_loss: 0.4337 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 78.
[32m[05/17 06:44:23 graph]: [0m# Epoch 78: train_loss: 0.4325 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 79.
[32m[05/17 06:44:23 graph]: [0m# Epoch 79: train_loss: 0.4319 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 80.
[32m[05/17 06:44:23 graph]: [0m# Epoch 80: train_loss: 0.4316 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 81.
[32m[05/17 06:44:23 graph]: [0m# Epoch 81: train_loss: 0.4295 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 82.
[32m[05/17 06:44:23 graph]: [0m# Epoch 82: train_loss: 0.4303 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 83.
[32m[05/17 06:44:23 graph]: [0m# Epoch 83: train_loss: 0.4287 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 84.
[32m[05/17 06:44:23 graph]: [0m# Epoch 84: train_loss: 0.4271 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 85.
[32m[05/17 06:44:23 graph]: [0m# Epoch 85: train_loss: 0.4283 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 86.
[32m[05/17 06:44:23 graph]: [0m# Epoch 86: train_loss: 0.4261 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 87.
[32m[05/17 06:44:23 graph]: [0m# Epoch 87: train_loss: 0.4257 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 88.
[32m[05/17 06:44:23 graph]: [0m# Epoch 88: train_loss: 0.4261 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 89.
[32m[05/17 06:44:23 graph]: [0m# Epoch 89: train_loss: 0.4242 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 90.
[32m[05/17 06:44:23 graph]: [0m# Epoch 90: train_loss: 0.4237 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 91.
[32m[05/17 06:44:23 graph]: [0m# Epoch 91: train_loss: 0.4241 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 92.
[32m[05/17 06:44:23 graph]: [0m# Epoch 92: train_loss: 0.4236 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 93.
[32m[05/17 06:44:23 graph]: [0m# Epoch 93: train_loss: 0.4225 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 94.
[32m[05/17 06:44:23 graph]: [0m# Epoch 94: train_loss: 0.4224 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 95.
[32m[05/17 06:44:23 graph]: [0m# Epoch 95: train_loss: 0.4211 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 96.
[32m[05/17 06:44:23 graph]: [0m# Epoch 96: train_loss: 0.4212 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 97.
[32m[05/17 06:44:23 graph]: [0m# Epoch 97: train_loss: 0.4206 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 98.
[32m[05/17 06:44:23 graph]: [0m# Epoch 98: train_loss: 0.4200 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 99.
[32m[05/17 06:44:23 graph]: [0m# Epoch 99: train_loss: 0.4191 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 100.
[32m[05/17 06:44:23 graph]: [0m# Epoch 100: train_loss: 0.4190 | lr: 0.000411
[32m[05/17 06:44:23 graph]: [0mstart epoch 101.
[32m[05/17 06:44:23 graph]: [0m# Epoch 101: train_loss: 0.4194 | lr: 0.000411
[32m[05/17 06:44:25 graph]: [0m--- Testf1: 0.92712, Best Valacc: 0.93072 in epoch 99 --- 
[32m[05/17 06:44:25 graph]: [0mEpoch 101: get test acc score:  0.927
[32m[05/17 06:44:25 graph]: [0mbest_f1 0.927 at epoch 101
[32m[05/17 06:44:25 graph]: [0mstart epoch 102.
[32m[05/17 06:44:25 graph]: [0m# Epoch 102: train_loss: 0.4196 | lr: 0.000411
[32m[05/17 06:44:26 graph]: [0m--- Testf1: 0.92304, Best Valacc: 0.93072 in epoch 45 --- 
[32m[05/17 06:44:26 graph]: [0mEpoch 102: get test acc score:  0.923
[32m[05/17 06:44:26 graph]: [0mbest_f1 0.927 at epoch 101
[32m[05/17 06:44:26 graph]: [0mstart epoch 103.
[32m[05/17 06:44:26 graph]: [0m# Epoch 103: train_loss: 0.4189 | lr: 0.000411
[32m[05/17 06:44:27 graph]: [0m--- Testf1: 0.92533, Best Valacc: 0.92418 in epoch 99 --- 
[32m[05/17 06:44:27 graph]: [0mEpoch 103: get test acc score:  0.925
[32m[05/17 06:44:27 graph]: [0mbest_f1 0.927 at epoch 101
[32m[05/17 06:44:27 graph]: [0mstart epoch 104.
[32m[05/17 06:44:27 graph]: [0m# Epoch 104: train_loss: 0.4182 | lr: 0.000411
[32m[05/17 06:44:28 graph]: [0m--- Testf1: 0.92680, Best Valacc: 0.92941 in epoch 99 --- 
[32m[05/17 06:44:28 graph]: [0mEpoch 104: get test acc score:  0.927
[32m[05/17 06:44:28 graph]: [0mbest_f1 0.927 at epoch 101
[32m[05/17 06:44:28 graph]: [0mstart epoch 105.
[32m[05/17 06:44:28 graph]: [0m# Epoch 105: train_loss: 0.4181 | lr: 0.000411
[32m[05/17 06:45:04 graph]: [0mRank of current process: 0. World size: 1
[32m[05/17 06:45:08 graph]: [0mEnvironment info:
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-90-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 495.44
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==1.10.0+cu113
[pip3] torchvision==0.11.1+cu113
[conda] blas                      1.0                         mkl    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl                       2021.4.0           h06a4308_640    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0            py38h7f8727e_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.1            py38hd3c417c_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.2            py38h51133e4_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.21.4                   pypi_0    pypi
[conda] numpy-base                1.23.5           py38h31eccc5_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] torch                     1.10.0+cu113             pypi_0    pypi
[conda] torchvision               0.11.1+cu113             pypi_0    pypi
        Pillow (8.4.0)
[32m[05/17 06:45:08 graph]: [0mCommand line arguments: Namespace(local_rank=0, resume=False, seed=1234, start_epoch=0)
[32m[05/17 06:45:08 graph]: [0mRun 0th for seed 0
[32m[05/17 06:45:08 graph]: [0mTotal trainable params num : 7813551
[32m[05/17 06:45:12 graph]: [0m----------Start Training----------
[32m[05/17 06:45:12 graph]: [0mstart epoch 0.
[32m[05/17 06:45:13 graph]: [0m# Epoch 0: train_loss: 1.0220 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 1.
[32m[05/17 06:45:13 graph]: [0m# Epoch 1: train_loss: 0.8043 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 2.
[32m[05/17 06:45:13 graph]: [0m# Epoch 2: train_loss: 0.6895 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 3.
[32m[05/17 06:45:13 graph]: [0m# Epoch 3: train_loss: 0.6363 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 4.
[32m[05/17 06:45:13 graph]: [0m# Epoch 4: train_loss: 0.6087 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 5.
[32m[05/17 06:45:13 graph]: [0m# Epoch 5: train_loss: 0.5906 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 6.
[32m[05/17 06:45:13 graph]: [0m# Epoch 6: train_loss: 0.5791 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 7.
[32m[05/17 06:45:13 graph]: [0m# Epoch 7: train_loss: 0.5704 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 8.
[32m[05/17 06:45:13 graph]: [0m# Epoch 8: train_loss: 0.5629 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 9.
[32m[05/17 06:45:13 graph]: [0m# Epoch 9: train_loss: 0.5555 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 10.
[32m[05/17 06:45:13 graph]: [0m# Epoch 10: train_loss: 0.5471 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 11.
[32m[05/17 06:45:13 graph]: [0m# Epoch 11: train_loss: 0.5397 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 12.
[32m[05/17 06:45:13 graph]: [0m# Epoch 12: train_loss: 0.5321 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 13.
[32m[05/17 06:45:13 graph]: [0m# Epoch 13: train_loss: 0.5266 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 14.
[32m[05/17 06:45:13 graph]: [0m# Epoch 14: train_loss: 0.5206 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 15.
[32m[05/17 06:45:13 graph]: [0m# Epoch 15: train_loss: 0.5168 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 16.
[32m[05/17 06:45:13 graph]: [0m# Epoch 16: train_loss: 0.5125 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 17.
[32m[05/17 06:45:13 graph]: [0m# Epoch 17: train_loss: 0.5083 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 18.
[32m[05/17 06:45:13 graph]: [0m# Epoch 18: train_loss: 0.5040 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 19.
[32m[05/17 06:45:13 graph]: [0m# Epoch 19: train_loss: 0.5003 | lr: 0.000411
[32m[05/17 06:45:13 graph]: [0mstart epoch 20.
[32m[05/17 06:45:14 graph]: [0m# Epoch 20: train_loss: 0.4973 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 21.
[32m[05/17 06:45:14 graph]: [0m# Epoch 21: train_loss: 0.4947 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 22.
[32m[05/17 06:45:14 graph]: [0m# Epoch 22: train_loss: 0.4916 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 23.
[32m[05/17 06:45:14 graph]: [0m# Epoch 23: train_loss: 0.4892 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 24.
[32m[05/17 06:45:14 graph]: [0m# Epoch 24: train_loss: 0.4859 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 25.
[32m[05/17 06:45:14 graph]: [0m# Epoch 25: train_loss: 0.4834 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 26.
[32m[05/17 06:45:14 graph]: [0m# Epoch 26: train_loss: 0.4806 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 27.
[32m[05/17 06:45:14 graph]: [0m# Epoch 27: train_loss: 0.4781 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 28.
[32m[05/17 06:45:14 graph]: [0m# Epoch 28: train_loss: 0.4776 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 29.
[32m[05/17 06:45:14 graph]: [0m# Epoch 29: train_loss: 0.4745 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 30.
[32m[05/17 06:45:14 graph]: [0m# Epoch 30: train_loss: 0.4722 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 31.
[32m[05/17 06:45:14 graph]: [0m# Epoch 31: train_loss: 0.4697 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 32.
[32m[05/17 06:45:14 graph]: [0m# Epoch 32: train_loss: 0.4682 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 33.
[32m[05/17 06:45:14 graph]: [0m# Epoch 33: train_loss: 0.4668 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 34.
[32m[05/17 06:45:14 graph]: [0m# Epoch 34: train_loss: 0.4645 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 35.
[32m[05/17 06:45:14 graph]: [0m# Epoch 35: train_loss: 0.4638 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 36.
[32m[05/17 06:45:14 graph]: [0m# Epoch 36: train_loss: 0.4636 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 37.
[32m[05/17 06:45:14 graph]: [0m# Epoch 37: train_loss: 0.4603 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 38.
[32m[05/17 06:45:14 graph]: [0m# Epoch 38: train_loss: 0.4605 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 39.
[32m[05/17 06:45:14 graph]: [0m# Epoch 39: train_loss: 0.4597 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 40.
[32m[05/17 06:45:14 graph]: [0m# Epoch 40: train_loss: 0.4575 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 41.
[32m[05/17 06:45:14 graph]: [0m# Epoch 41: train_loss: 0.4592 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 42.
[32m[05/17 06:45:14 graph]: [0m# Epoch 42: train_loss: 0.4567 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 43.
[32m[05/17 06:45:14 graph]: [0m# Epoch 43: train_loss: 0.4560 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 44.
[32m[05/17 06:45:14 graph]: [0m# Epoch 44: train_loss: 0.4542 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 45.
[32m[05/17 06:45:14 graph]: [0m# Epoch 45: train_loss: 0.4538 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 46.
[32m[05/17 06:45:14 graph]: [0m# Epoch 46: train_loss: 0.4532 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 47.
[32m[05/17 06:45:14 graph]: [0m# Epoch 47: train_loss: 0.4522 | lr: 0.000411
[32m[05/17 06:45:14 graph]: [0mstart epoch 48.
[32m[05/17 06:45:15 graph]: [0m# Epoch 48: train_loss: 0.4511 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 49.
[32m[05/17 06:45:15 graph]: [0m# Epoch 49: train_loss: 0.4512 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 50.
[32m[05/17 06:45:15 graph]: [0m# Epoch 50: train_loss: 0.4500 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 51.
[32m[05/17 06:45:15 graph]: [0m# Epoch 51: train_loss: 0.4494 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 52.
[32m[05/17 06:45:15 graph]: [0m# Epoch 52: train_loss: 0.4480 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 53.
[32m[05/17 06:45:15 graph]: [0m# Epoch 53: train_loss: 0.4483 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 54.
[32m[05/17 06:45:15 graph]: [0m# Epoch 54: train_loss: 0.4463 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 55.
[32m[05/17 06:45:15 graph]: [0m# Epoch 55: train_loss: 0.4468 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 56.
[32m[05/17 06:45:15 graph]: [0m# Epoch 56: train_loss: 0.4450 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 57.
[32m[05/17 06:45:15 graph]: [0m# Epoch 57: train_loss: 0.4453 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 58.
[32m[05/17 06:45:15 graph]: [0m# Epoch 58: train_loss: 0.4440 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 59.
[32m[05/17 06:45:15 graph]: [0m# Epoch 59: train_loss: 0.4435 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 60.
[32m[05/17 06:45:15 graph]: [0m# Epoch 60: train_loss: 0.4424 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 61.
[32m[05/17 06:45:15 graph]: [0m# Epoch 61: train_loss: 0.4421 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 62.
[32m[05/17 06:45:15 graph]: [0m# Epoch 62: train_loss: 0.4412 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 63.
[32m[05/17 06:45:15 graph]: [0m# Epoch 63: train_loss: 0.4402 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 64.
[32m[05/17 06:45:15 graph]: [0m# Epoch 64: train_loss: 0.4402 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 65.
[32m[05/17 06:45:15 graph]: [0m# Epoch 65: train_loss: 0.4403 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 66.
[32m[05/17 06:45:15 graph]: [0m# Epoch 66: train_loss: 0.4466 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 67.
[32m[05/17 06:45:15 graph]: [0m# Epoch 67: train_loss: 0.4460 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 68.
[32m[05/17 06:45:15 graph]: [0m# Epoch 68: train_loss: 0.4405 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 69.
[32m[05/17 06:45:15 graph]: [0m# Epoch 69: train_loss: 0.4404 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 70.
[32m[05/17 06:45:15 graph]: [0m# Epoch 70: train_loss: 0.4404 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 71.
[32m[05/17 06:45:15 graph]: [0m# Epoch 71: train_loss: 0.4371 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 72.
[32m[05/17 06:45:15 graph]: [0m# Epoch 72: train_loss: 0.4385 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 73.
[32m[05/17 06:45:15 graph]: [0m# Epoch 73: train_loss: 0.4379 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 74.
[32m[05/17 06:45:15 graph]: [0m# Epoch 74: train_loss: 0.4353 | lr: 0.000411
[32m[05/17 06:45:15 graph]: [0mstart epoch 75.
[32m[05/17 06:45:16 graph]: [0m# Epoch 75: train_loss: 0.4363 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 76.
[32m[05/17 06:45:16 graph]: [0m# Epoch 76: train_loss: 0.4346 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 77.
[32m[05/17 06:45:16 graph]: [0m# Epoch 77: train_loss: 0.4340 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 78.
[32m[05/17 06:45:16 graph]: [0m# Epoch 78: train_loss: 0.4326 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 79.
[32m[05/17 06:45:16 graph]: [0m# Epoch 79: train_loss: 0.4316 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 80.
[32m[05/17 06:45:16 graph]: [0m# Epoch 80: train_loss: 0.4310 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 81.
[32m[05/17 06:45:16 graph]: [0m# Epoch 81: train_loss: 0.4298 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 82.
[32m[05/17 06:45:16 graph]: [0m# Epoch 82: train_loss: 0.4295 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 83.
[32m[05/17 06:45:16 graph]: [0m# Epoch 83: train_loss: 0.4288 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 84.
[32m[05/17 06:45:16 graph]: [0m# Epoch 84: train_loss: 0.4278 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 85.
[32m[05/17 06:45:16 graph]: [0m# Epoch 85: train_loss: 0.4274 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 86.
[32m[05/17 06:45:16 graph]: [0m# Epoch 86: train_loss: 0.4271 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 87.
[32m[05/17 06:45:16 graph]: [0m# Epoch 87: train_loss: 0.4264 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 88.
[32m[05/17 06:45:16 graph]: [0m# Epoch 88: train_loss: 0.4264 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 89.
[32m[05/17 06:45:16 graph]: [0m# Epoch 89: train_loss: 0.4287 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 90.
[32m[05/17 06:45:16 graph]: [0m# Epoch 90: train_loss: 0.4251 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 91.
[32m[05/17 06:45:16 graph]: [0m# Epoch 91: train_loss: 0.4260 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 92.
[32m[05/17 06:45:16 graph]: [0m# Epoch 92: train_loss: 0.4246 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 93.
[32m[05/17 06:45:16 graph]: [0m# Epoch 93: train_loss: 0.4239 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 94.
[32m[05/17 06:45:16 graph]: [0m# Epoch 94: train_loss: 0.4237 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 95.
[32m[05/17 06:45:16 graph]: [0m# Epoch 95: train_loss: 0.4221 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 96.
[32m[05/17 06:45:16 graph]: [0m# Epoch 96: train_loss: 0.4219 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 97.
[32m[05/17 06:45:16 graph]: [0m# Epoch 97: train_loss: 0.4212 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 98.
[32m[05/17 06:45:16 graph]: [0m# Epoch 98: train_loss: 0.4209 | lr: 0.000411
[32m[05/17 06:45:16 graph]: [0mstart epoch 99.
[32m[05/17 06:45:16 graph]: [0m# Epoch 99: train_loss: 0.4202 | lr: 0.000411
[32m[05/17 06:45:39 graph]: [0mRank of current process: 0. World size: 1
[32m[05/17 06:45:43 graph]: [0mEnvironment info:
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-90-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 495.44
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==1.10.0+cu113
[pip3] torchvision==0.11.1+cu113
[conda] blas                      1.0                         mkl    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl                       2021.4.0           h06a4308_640    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0            py38h7f8727e_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.1            py38hd3c417c_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.2            py38h51133e4_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.21.4                   pypi_0    pypi
[conda] numpy-base                1.23.5           py38h31eccc5_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] torch                     1.10.0+cu113             pypi_0    pypi
[conda] torchvision               0.11.1+cu113             pypi_0    pypi
        Pillow (8.4.0)
[32m[05/17 06:45:43 graph]: [0mCommand line arguments: Namespace(local_rank=0, resume=False, seed=1234, start_epoch=0)
[32m[05/17 06:45:43 graph]: [0mRun 0th for seed 0
[32m[05/17 06:45:43 graph]: [0mTotal trainable params num : 7813551
[32m[05/17 06:45:47 graph]: [0m----------Start Training----------
[32m[05/17 06:45:47 graph]: [0mstart epoch 0.
[32m[05/17 06:45:47 graph]: [0m# Epoch 0: train_loss: 1.0220 | lr: 0.000411
[32m[05/17 06:45:47 graph]: [0mstart epoch 1.
[32m[05/17 06:45:47 graph]: [0m# Epoch 1: train_loss: 0.8043 | lr: 0.000411
[32m[05/17 06:45:47 graph]: [0mstart epoch 2.
[32m[05/17 06:45:47 graph]: [0m# Epoch 2: train_loss: 0.6895 | lr: 0.000411
[32m[05/17 06:45:47 graph]: [0mstart epoch 3.
[32m[05/17 06:45:47 graph]: [0m# Epoch 3: train_loss: 0.6362 | lr: 0.000411
[32m[05/17 06:45:47 graph]: [0mstart epoch 4.
[32m[05/17 06:45:48 graph]: [0m# Epoch 4: train_loss: 0.6087 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 5.
[32m[05/17 06:45:48 graph]: [0m# Epoch 5: train_loss: 0.5906 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 6.
[32m[05/17 06:45:48 graph]: [0m# Epoch 6: train_loss: 0.5791 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 7.
[32m[05/17 06:45:48 graph]: [0m# Epoch 7: train_loss: 0.5704 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 8.
[32m[05/17 06:45:48 graph]: [0m# Epoch 8: train_loss: 0.5629 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 9.
[32m[05/17 06:45:48 graph]: [0m# Epoch 9: train_loss: 0.5555 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 10.
[32m[05/17 06:45:48 graph]: [0m# Epoch 10: train_loss: 0.5470 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 11.
[32m[05/17 06:45:48 graph]: [0m# Epoch 11: train_loss: 0.5397 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 12.
[32m[05/17 06:45:48 graph]: [0m# Epoch 12: train_loss: 0.5321 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 13.
[32m[05/17 06:45:48 graph]: [0m# Epoch 13: train_loss: 0.5265 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 14.
[32m[05/17 06:45:48 graph]: [0m# Epoch 14: train_loss: 0.5206 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 15.
[32m[05/17 06:45:48 graph]: [0m# Epoch 15: train_loss: 0.5168 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 16.
[32m[05/17 06:45:48 graph]: [0m# Epoch 16: train_loss: 0.5125 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 17.
[32m[05/17 06:45:48 graph]: [0m# Epoch 17: train_loss: 0.5083 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 18.
[32m[05/17 06:45:48 graph]: [0m# Epoch 18: train_loss: 0.5040 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 19.
[32m[05/17 06:45:48 graph]: [0m# Epoch 19: train_loss: 0.5003 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 20.
[32m[05/17 06:45:48 graph]: [0m# Epoch 20: train_loss: 0.4973 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 21.
[32m[05/17 06:45:48 graph]: [0m# Epoch 21: train_loss: 0.4947 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 22.
[32m[05/17 06:45:48 graph]: [0m# Epoch 22: train_loss: 0.4916 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 23.
[32m[05/17 06:45:48 graph]: [0m# Epoch 23: train_loss: 0.4892 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 24.
[32m[05/17 06:45:48 graph]: [0m# Epoch 24: train_loss: 0.4858 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 25.
[32m[05/17 06:45:48 graph]: [0m# Epoch 25: train_loss: 0.4834 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 26.
[32m[05/17 06:45:48 graph]: [0m# Epoch 26: train_loss: 0.4807 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 27.
[32m[05/17 06:45:48 graph]: [0m# Epoch 27: train_loss: 0.4782 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 28.
[32m[05/17 06:45:48 graph]: [0m# Epoch 28: train_loss: 0.4777 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 29.
[32m[05/17 06:45:48 graph]: [0m# Epoch 29: train_loss: 0.4741 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 30.
[32m[05/17 06:45:48 graph]: [0m# Epoch 30: train_loss: 0.4715 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 31.
[32m[05/17 06:45:48 graph]: [0m# Epoch 31: train_loss: 0.4697 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 32.
[32m[05/17 06:45:48 graph]: [0m# Epoch 32: train_loss: 0.4677 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 33.
[32m[05/17 06:45:48 graph]: [0m# Epoch 33: train_loss: 0.4660 | lr: 0.000411
[32m[05/17 06:45:48 graph]: [0mstart epoch 34.
[32m[05/17 06:45:49 graph]: [0m# Epoch 34: train_loss: 0.4645 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 35.
[32m[05/17 06:45:49 graph]: [0m# Epoch 35: train_loss: 0.4628 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 36.
[32m[05/17 06:45:49 graph]: [0m# Epoch 36: train_loss: 0.4616 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 37.
[32m[05/17 06:45:49 graph]: [0m# Epoch 37: train_loss: 0.4607 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 38.
[32m[05/17 06:45:49 graph]: [0m# Epoch 38: train_loss: 0.4610 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 39.
[32m[05/17 06:45:49 graph]: [0m# Epoch 39: train_loss: 0.4602 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 40.
[32m[05/17 06:45:49 graph]: [0m# Epoch 40: train_loss: 0.4600 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 41.
[32m[05/17 06:45:49 graph]: [0m# Epoch 41: train_loss: 0.4563 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 42.
[32m[05/17 06:45:49 graph]: [0m# Epoch 42: train_loss: 0.4599 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 43.
[32m[05/17 06:45:49 graph]: [0m# Epoch 43: train_loss: 0.4560 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 44.
[32m[05/17 06:45:49 graph]: [0m# Epoch 44: train_loss: 0.4566 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 45.
[32m[05/17 06:45:49 graph]: [0m# Epoch 45: train_loss: 0.4533 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 46.
[32m[05/17 06:45:49 graph]: [0m# Epoch 46: train_loss: 0.4542 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 47.
[32m[05/17 06:45:49 graph]: [0m# Epoch 47: train_loss: 0.4521 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 48.
[32m[05/17 06:45:49 graph]: [0m# Epoch 48: train_loss: 0.4518 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 49.
[32m[05/17 06:45:49 graph]: [0m# Epoch 49: train_loss: 0.4504 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 50.
[32m[05/17 06:45:49 graph]: [0m# Epoch 50: train_loss: 0.4505 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 51.
[32m[05/17 06:45:49 graph]: [0m# Epoch 51: train_loss: 0.4492 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 52.
[32m[05/17 06:45:49 graph]: [0m# Epoch 52: train_loss: 0.4486 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 53.
[32m[05/17 06:45:49 graph]: [0m# Epoch 53: train_loss: 0.4480 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 54.
[32m[05/17 06:45:49 graph]: [0m# Epoch 54: train_loss: 0.4466 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 55.
[32m[05/17 06:45:49 graph]: [0m# Epoch 55: train_loss: 0.4468 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 56.
[32m[05/17 06:45:49 graph]: [0m# Epoch 56: train_loss: 0.4452 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 57.
[32m[05/17 06:45:49 graph]: [0m# Epoch 57: train_loss: 0.4453 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 58.
[32m[05/17 06:45:49 graph]: [0m# Epoch 58: train_loss: 0.4440 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 59.
[32m[05/17 06:45:49 graph]: [0m# Epoch 59: train_loss: 0.4442 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 60.
[32m[05/17 06:45:49 graph]: [0m# Epoch 60: train_loss: 0.4425 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 61.
[32m[05/17 06:45:49 graph]: [0m# Epoch 61: train_loss: 0.4428 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 62.
[32m[05/17 06:45:49 graph]: [0m# Epoch 62: train_loss: 0.4416 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 63.
[32m[05/17 06:45:49 graph]: [0m# Epoch 63: train_loss: 0.4409 | lr: 0.000411
[32m[05/17 06:45:49 graph]: [0mstart epoch 64.
[32m[05/17 06:45:50 graph]: [0m# Epoch 64: train_loss: 0.4407 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 65.
[32m[05/17 06:45:50 graph]: [0m# Epoch 65: train_loss: 0.4410 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 66.
[32m[05/17 06:45:50 graph]: [0m# Epoch 66: train_loss: 0.4422 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 67.
[32m[05/17 06:45:50 graph]: [0m# Epoch 67: train_loss: 0.4426 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 68.
[32m[05/17 06:45:50 graph]: [0m# Epoch 68: train_loss: 0.4391 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 69.
[32m[05/17 06:45:50 graph]: [0m# Epoch 69: train_loss: 0.4395 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 70.
[32m[05/17 06:45:50 graph]: [0m# Epoch 70: train_loss: 0.4381 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 71.
[32m[05/17 06:45:50 graph]: [0m# Epoch 71: train_loss: 0.4359 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 72.
[32m[05/17 06:45:50 graph]: [0m# Epoch 72: train_loss: 0.4367 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 73.
[32m[05/17 06:45:50 graph]: [0m# Epoch 73: train_loss: 0.4344 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 74.
[32m[05/17 06:45:50 graph]: [0m# Epoch 74: train_loss: 0.4355 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 75.
[32m[05/17 06:45:50 graph]: [0m# Epoch 75: train_loss: 0.4346 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 76.
[32m[05/17 06:45:50 graph]: [0m# Epoch 76: train_loss: 0.4331 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 77.
[32m[05/17 06:45:50 graph]: [0m# Epoch 77: train_loss: 0.4324 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 78.
[32m[05/17 06:45:50 graph]: [0m# Epoch 78: train_loss: 0.4318 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 79.
[32m[05/17 06:45:50 graph]: [0m# Epoch 79: train_loss: 0.4311 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 80.
[32m[05/17 06:45:50 graph]: [0m# Epoch 80: train_loss: 0.4302 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 81.
[32m[05/17 06:45:50 graph]: [0m# Epoch 81: train_loss: 0.4295 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 82.
[32m[05/17 06:45:50 graph]: [0m# Epoch 82: train_loss: 0.4288 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 83.
[32m[05/17 06:45:50 graph]: [0m# Epoch 83: train_loss: 0.4285 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 84.
[32m[05/17 06:45:50 graph]: [0m# Epoch 84: train_loss: 0.4271 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 85.
[32m[05/17 06:45:50 graph]: [0m# Epoch 85: train_loss: 0.4268 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 86.
[32m[05/17 06:45:50 graph]: [0m# Epoch 86: train_loss: 0.4262 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 87.
[32m[05/17 06:45:50 graph]: [0m# Epoch 87: train_loss: 0.4255 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 88.
[32m[05/17 06:45:50 graph]: [0m# Epoch 88: train_loss: 0.4256 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 89.
[32m[05/17 06:45:50 graph]: [0m# Epoch 89: train_loss: 0.4257 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 90.
[32m[05/17 06:45:50 graph]: [0m# Epoch 90: train_loss: 0.4255 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 91.
[32m[05/17 06:45:50 graph]: [0m# Epoch 91: train_loss: 0.4248 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 92.
[32m[05/17 06:45:50 graph]: [0m# Epoch 92: train_loss: 0.4239 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 93.
[32m[05/17 06:45:50 graph]: [0m# Epoch 93: train_loss: 0.4232 | lr: 0.000411
[32m[05/17 06:45:50 graph]: [0mstart epoch 94.
[32m[05/17 06:45:51 graph]: [0m# Epoch 94: train_loss: 0.4223 | lr: 0.000411
[32m[05/17 06:45:51 graph]: [0mstart epoch 95.
[32m[05/17 06:45:51 graph]: [0m# Epoch 95: train_loss: 0.4219 | lr: 0.000411
[32m[05/17 06:45:51 graph]: [0mstart epoch 96.
[32m[05/17 06:45:51 graph]: [0m# Epoch 96: train_loss: 0.4211 | lr: 0.000411
[32m[05/17 06:45:51 graph]: [0mstart epoch 97.
[32m[05/17 06:45:51 graph]: [0m# Epoch 97: train_loss: 0.4209 | lr: 0.000411
[32m[05/17 06:45:51 graph]: [0mstart epoch 98.
[32m[05/17 06:45:51 graph]: [0m# Epoch 98: train_loss: 0.4201 | lr: 0.000411
[32m[05/17 06:45:51 graph]: [0mstart epoch 99.
[32m[05/17 06:45:51 graph]: [0m# Epoch 99: train_loss: 0.4196 | lr: 0.000411
[32m[05/17 06:45:51 graph]: [0m# final_acc: -infÂ±nan
[32m[05/17 06:47:22 graph]: [0mRank of current process: 0. World size: 1
[32m[05/17 06:47:25 graph]: [0mEnvironment info:
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-90-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 495.44
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==1.10.0+cu113
[pip3] torchvision==0.11.1+cu113
[conda] blas                      1.0                         mkl    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl                       2021.4.0           h06a4308_640    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0            py38h7f8727e_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.1            py38hd3c417c_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.2            py38h51133e4_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.21.4                   pypi_0    pypi
[conda] numpy-base                1.23.5           py38h31eccc5_0    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] torch                     1.10.0+cu113             pypi_0    pypi
[conda] torchvision               0.11.1+cu113             pypi_0    pypi
        Pillow (8.4.0)
[32m[05/17 06:47:25 graph]: [0mCommand line arguments: Namespace(local_rank=0, resume=False, seed=1234, start_epoch=0)
[32m[05/17 06:47:25 graph]: [0mRun 0th for seed 0
[32m[05/17 06:47:25 graph]: [0mTotal trainable params num : 7813551
[32m[05/17 06:47:30 graph]: [0m----------Start Training----------
[32m[05/17 06:47:30 graph]: [0mstart epoch 0.
[32m[05/17 06:47:30 graph]: [0m# Epoch 0: train_loss: 1.0177 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 1.
[32m[05/17 06:47:30 graph]: [0m# Epoch 1: train_loss: 0.8020 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 2.
[32m[05/17 06:47:30 graph]: [0m# Epoch 2: train_loss: 0.6886 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 3.
[32m[05/17 06:47:30 graph]: [0m# Epoch 3: train_loss: 0.6398 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 4.
[32m[05/17 06:47:30 graph]: [0m# Epoch 4: train_loss: 0.6133 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 5.
[32m[05/17 06:47:30 graph]: [0m# Epoch 5: train_loss: 0.5980 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 6.
[32m[05/17 06:47:30 graph]: [0m# Epoch 6: train_loss: 0.5884 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 7.
[32m[05/17 06:47:30 graph]: [0m# Epoch 7: train_loss: 0.5816 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 8.
[32m[05/17 06:47:30 graph]: [0m# Epoch 8: train_loss: 0.5765 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 9.
[32m[05/17 06:47:30 graph]: [0m# Epoch 9: train_loss: 0.5721 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 10.
[32m[05/17 06:47:30 graph]: [0m# Epoch 10: train_loss: 0.5679 | lr: 0.000411
[32m[05/17 06:47:30 graph]: [0mstart epoch 11.
[32m[05/17 06:47:31 graph]: [0m# Epoch 11: train_loss: 0.5645 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 12.
[32m[05/17 06:47:31 graph]: [0m# Epoch 12: train_loss: 0.5612 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 13.
[32m[05/17 06:47:31 graph]: [0m# Epoch 13: train_loss: 0.5584 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 14.
[32m[05/17 06:47:31 graph]: [0m# Epoch 14: train_loss: 0.5562 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 15.
[32m[05/17 06:47:31 graph]: [0m# Epoch 15: train_loss: 0.5544 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 16.
[32m[05/17 06:47:31 graph]: [0m# Epoch 16: train_loss: 0.5533 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 17.
[32m[05/17 06:47:31 graph]: [0m# Epoch 17: train_loss: 0.5523 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 18.
[32m[05/17 06:47:31 graph]: [0m# Epoch 18: train_loss: 0.5518 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 19.
[32m[05/17 06:47:31 graph]: [0m# Epoch 19: train_loss: 0.5510 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 20.
[32m[05/17 06:47:31 graph]: [0m# Epoch 20: train_loss: 0.5506 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 21.
[32m[05/17 06:47:31 graph]: [0m# Epoch 21: train_loss: 0.5500 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 22.
[32m[05/17 06:47:31 graph]: [0m# Epoch 22: train_loss: 0.5495 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 23.
[32m[05/17 06:47:31 graph]: [0m# Epoch 23: train_loss: 0.5488 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 24.
[32m[05/17 06:47:31 graph]: [0m# Epoch 24: train_loss: 0.5483 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 25.
[32m[05/17 06:47:31 graph]: [0m# Epoch 25: train_loss: 0.5480 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 26.
[32m[05/17 06:47:31 graph]: [0m# Epoch 26: train_loss: 0.5474 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 27.
[32m[05/17 06:47:31 graph]: [0m# Epoch 27: train_loss: 0.5471 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 28.
[32m[05/17 06:47:31 graph]: [0m# Epoch 28: train_loss: 0.5464 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 29.
[32m[05/17 06:47:31 graph]: [0m# Epoch 29: train_loss: 0.5458 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 30.
[32m[05/17 06:47:31 graph]: [0m# Epoch 30: train_loss: 0.5454 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 31.
[32m[05/17 06:47:31 graph]: [0m# Epoch 31: train_loss: 0.5444 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 32.
[32m[05/17 06:47:31 graph]: [0m# Epoch 32: train_loss: 0.5436 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 33.
[32m[05/17 06:47:31 graph]: [0m# Epoch 33: train_loss: 0.5419 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 34.
[32m[05/17 06:47:31 graph]: [0m# Epoch 34: train_loss: 0.5395 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 35.
[32m[05/17 06:47:31 graph]: [0m# Epoch 35: train_loss: 0.5368 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 36.
[32m[05/17 06:47:31 graph]: [0m# Epoch 36: train_loss: 0.5341 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 37.
[32m[05/17 06:47:31 graph]: [0m# Epoch 37: train_loss: 0.5332 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 38.
[32m[05/17 06:47:31 graph]: [0m# Epoch 38: train_loss: 0.5300 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 39.
[32m[05/17 06:47:31 graph]: [0m# Epoch 39: train_loss: 0.5275 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 40.
[32m[05/17 06:47:31 graph]: [0m# Epoch 40: train_loss: 0.5268 | lr: 0.000411
[32m[05/17 06:47:31 graph]: [0mstart epoch 41.
[32m[05/17 06:47:32 graph]: [0m# Epoch 41: train_loss: 0.5221 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 42.
[32m[05/17 06:47:32 graph]: [0m# Epoch 42: train_loss: 0.5258 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 43.
[32m[05/17 06:47:32 graph]: [0m# Epoch 43: train_loss: 0.5199 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 44.
[32m[05/17 06:47:32 graph]: [0m# Epoch 44: train_loss: 0.5214 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 45.
[32m[05/17 06:47:32 graph]: [0m# Epoch 45: train_loss: 0.5184 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 46.
[32m[05/17 06:47:32 graph]: [0m# Epoch 46: train_loss: 0.5146 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 47.
[32m[05/17 06:47:32 graph]: [0m# Epoch 47: train_loss: 0.5132 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 48.
[32m[05/17 06:47:32 graph]: [0m# Epoch 48: train_loss: 0.5118 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 49.
[32m[05/17 06:47:32 graph]: [0m# Epoch 49: train_loss: 0.5063 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 50.
[32m[05/17 06:47:32 graph]: [0m# Epoch 50: train_loss: 0.5049 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 51.
[32m[05/17 06:47:32 graph]: [0m# Epoch 51: train_loss: 0.5030 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 52.
[32m[05/17 06:47:32 graph]: [0m# Epoch 52: train_loss: 0.5030 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 53.
[32m[05/17 06:47:32 graph]: [0m# Epoch 53: train_loss: 0.5012 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 54.
[32m[05/17 06:47:32 graph]: [0m# Epoch 54: train_loss: 0.4956 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 55.
[32m[05/17 06:47:32 graph]: [0m# Epoch 55: train_loss: 0.4951 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 56.
[32m[05/17 06:47:32 graph]: [0m# Epoch 56: train_loss: 0.4929 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 57.
[32m[05/17 06:47:32 graph]: [0m# Epoch 57: train_loss: 0.4917 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 58.
[32m[05/17 06:47:32 graph]: [0m# Epoch 58: train_loss: 0.4905 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 59.
[32m[05/17 06:47:32 graph]: [0m# Epoch 59: train_loss: 0.4881 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 60.
[32m[05/17 06:47:32 graph]: [0m# Epoch 60: train_loss: 0.4873 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 61.
[32m[05/17 06:47:32 graph]: [0m# Epoch 61: train_loss: 0.4862 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 62.
[32m[05/17 06:47:32 graph]: [0m# Epoch 62: train_loss: 0.4853 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 63.
[32m[05/17 06:47:32 graph]: [0m# Epoch 63: train_loss: 0.4838 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 64.
[32m[05/17 06:47:32 graph]: [0m# Epoch 64: train_loss: 0.4825 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 65.
[32m[05/17 06:47:32 graph]: [0m# Epoch 65: train_loss: 0.4813 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 66.
[32m[05/17 06:47:32 graph]: [0m# Epoch 66: train_loss: 0.4819 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 67.
[32m[05/17 06:47:32 graph]: [0m# Epoch 67: train_loss: 0.4782 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 68.
[32m[05/17 06:47:32 graph]: [0m# Epoch 68: train_loss: 0.4812 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 69.
[32m[05/17 06:47:32 graph]: [0m# Epoch 69: train_loss: 0.4776 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 70.
[32m[05/17 06:47:32 graph]: [0m# Epoch 70: train_loss: 0.4783 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 71.
[32m[05/17 06:47:32 graph]: [0m# Epoch 71: train_loss: 0.4736 | lr: 0.000411
[32m[05/17 06:47:32 graph]: [0mstart epoch 72.
[32m[05/17 06:47:33 graph]: [0m# Epoch 72: train_loss: 0.4755 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 73.
[32m[05/17 06:47:33 graph]: [0m# Epoch 73: train_loss: 0.4724 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 74.
[32m[05/17 06:47:33 graph]: [0m# Epoch 74: train_loss: 0.4724 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 75.
[32m[05/17 06:47:33 graph]: [0m# Epoch 75: train_loss: 0.4707 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 76.
[32m[05/17 06:47:33 graph]: [0m# Epoch 76: train_loss: 0.4694 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 77.
[32m[05/17 06:47:33 graph]: [0m# Epoch 77: train_loss: 0.4696 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 78.
[32m[05/17 06:47:33 graph]: [0m# Epoch 78: train_loss: 0.4679 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 79.
[32m[05/17 06:47:33 graph]: [0m# Epoch 79: train_loss: 0.4695 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 80.
[32m[05/17 06:47:33 graph]: [0m# Epoch 80: train_loss: 0.4664 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 81.
[32m[05/17 06:47:33 graph]: [0m# Epoch 81: train_loss: 0.4721 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 82.
[32m[05/17 06:47:33 graph]: [0m# Epoch 82: train_loss: 0.4646 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 83.
[32m[05/17 06:47:33 graph]: [0m# Epoch 83: train_loss: 0.4678 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 84.
[32m[05/17 06:47:33 graph]: [0m# Epoch 84: train_loss: 0.4644 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 85.
[32m[05/17 06:47:33 graph]: [0m# Epoch 85: train_loss: 0.4666 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 86.
[32m[05/17 06:47:33 graph]: [0m# Epoch 86: train_loss: 0.4647 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 87.
[32m[05/17 06:47:33 graph]: [0m# Epoch 87: train_loss: 0.4630 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 88.
[32m[05/17 06:47:33 graph]: [0m# Epoch 88: train_loss: 0.4634 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 89.
[32m[05/17 06:47:33 graph]: [0m# Epoch 89: train_loss: 0.4614 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 90.
[32m[05/17 06:47:33 graph]: [0m# Epoch 90: train_loss: 0.4621 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 91.
[32m[05/17 06:47:33 graph]: [0m# Epoch 91: train_loss: 0.4648 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 92.
[32m[05/17 06:47:33 graph]: [0m# Epoch 92: train_loss: 0.4604 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 93.
[32m[05/17 06:47:33 graph]: [0m# Epoch 93: train_loss: 0.4607 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 94.
[32m[05/17 06:47:33 graph]: [0m# Epoch 94: train_loss: 0.4614 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 95.
[32m[05/17 06:47:33 graph]: [0m# Epoch 95: train_loss: 0.4607 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 96.
[32m[05/17 06:47:33 graph]: [0m# Epoch 96: train_loss: 0.4630 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 97.
[32m[05/17 06:47:33 graph]: [0m# Epoch 97: train_loss: 0.4587 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 98.
[32m[05/17 06:47:33 graph]: [0m# Epoch 98: train_loss: 0.4628 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0mstart epoch 99.
[32m[05/17 06:47:33 graph]: [0m# Epoch 99: train_loss: 0.4572 | lr: 0.000411
[32m[05/17 06:47:33 graph]: [0m# final_acc: -infÂ±nan
